"""
Anne de Jong
2023 Dec

python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session classI            -mode A -query /data/bagel5/example/classI
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session classII           -mode A -query /data/bagel5/example/classII
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session classIII          -mode A -query /data/bagel5/example/classIII
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session classV            -mode A -query /data/bagel5/example/classV
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Linaridin         -mode A -query /data/bagel5/example/Linaridin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session LAP_Bottromycin   -mode A -query /data/bagel5/example/LAP_Bottromycin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Cyanobactins      -mode A -query /data/bagel5/example/Cyanobactins
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session D39               -mode A -query /data/bagel5/example/D39
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Epipeptide        -mode A -query /data/bagel5/example/Epipeptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Glycocin          -mode A -query /data/bagel5/example/Glycocin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Graspetide        -mode A -query /data/bagel5/example/Graspetide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session LAP_Bottromycin   -mode A -query /data/bagel5/example/LAP_Bottromycin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Lasso_peptide     -mode A -query /data/bagel5/example/Lasso_peptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Linaridin         -mode A -query /data/bagel5/example/Linaridin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session rSAM              -mode A -query /data/bagel5/example/rSAM
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Thioamitide       -mode A -query /data/bagel5/example/Thioamitide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Thiopeptide       -mode A -query /data/bagel5/example/Thiopeptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Thiopeptide_frag  -mode A -c3 False   -query /data/bagel5/example/Thiopeptide_frag
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Cyanobactin_frag1 -mode A -c3 False   -query /data/bagel5/example/Cyanobactin_frag1

python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation -session Cyanobactin_frag2 -mode A -c3 False   -query /data/bagel5/example/Cyanobactin_frag2


python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/classI            -mode D -query /data/bagel5/example/classI
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/classII           -mode D -query /data/bagel5/example/classII
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/classIII          -mode D -query /data/bagel5/example/classIII
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/classV            -mode D -query /data/bagel5/example/classV
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Linaridin         -mode D -query /data/bagel5/example/Linaridin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/LAP_Bottromycin   -mode D -query /data/bagel5/example/LAP_Bottromycin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Cyanobactins      -mode D -query /data/bagel5/example/Cyanobactins
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/D39               -mode D -query /data/bagel5/example/D39
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Epipeptide        -mode D -query /data/bagel5/example/Epipeptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Glycocin          -mode D -query /data/bagel5/example/Glycocin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Graspetide        -mode D -query /data/bagel5/example/Graspetide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/LAP_Bottromycin   -mode D -query /data/bagel5/example/LAP_Bottromycin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Lasso_peptide     -mode D -query /data/bagel5/example/Lasso_peptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Linaridin         -mode D -query /data/bagel5/example/Linaridin
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/rSAM              -mode D -query /data/bagel5/example/rSAM
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Thioamitide       -mode D -query /data/bagel5/example/Thioamitide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Thiopeptide       -mode D -query /data/bagel5/example/Thiopeptide
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Thiopeptide_frag  -mode D -c3 False   -query /data/bagel5/example/Thiopeptide_frag
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Cyanobactin_frag1 -mode D -c3 False   -query /data/bagel5/example/Cyanobactin_frag1
python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_discovery/Cyanobactin_frag2 -mode D -c3 False   -query /data/bagel5/example/Cyanobactin_frag2




python3 /data/bagel5/scripts/bagel5.py -outdir /data/www/users/vicky/bagel5_annotation/test -c3 False -query /data/bagel5/example/classI


"""

import sys
import pandas as pd
import argparse 
import subprocess
import os
import glob
import re
import numpy as np
from pepmatch import Matcher


# S='ABCDEFGHIJKLMNOPQ'
# Size=4
# for i in range(0, len(S), Size): 
# 	print(S[i:i+Size])
# 	
# exit()	


parser = argparse.ArgumentParser(description='BAGEL5 (by Anne de Jong and Vicky Fernandez Cantos')
parser.add_argument('-query', dest='queryfolder', help='Full path to the queryfolder with .fna files', nargs='?', default='.')
parser.add_argument('-outdir', dest='outdir', help='Results Folder [default=current folder]', nargs='?', default='.')
parser.add_argument('-session', dest='sessionid', help='session id [default=results]', nargs='?', default='results')
parser.add_argument('-cpu', type=str, dest='cpu', help='Number of threads [6]', nargs='?', default=6)
parser.add_argument('-domT', type=str, dest='domT', help='hmm_domTscore [10]', nargs='?', default=10)
parser.add_argument('-AOIsize', type=int, dest='AOIsize', help='AOI size [30000]', nargs='?', default=30000)
parser.add_argument('-minAOIsize', type=int, dest='minAOIsize', help='AOI size [5000]', nargs='?', default=5000)
parser.add_argument('-mode', dest='mode', help='Annotation mode [default=A] or Discovery mode [D] ', nargs='?', default='A')
parser.add_argument('-pepsize', type=int, dest='PepSize', help='max peptide size for lantibiotics [default=120]', nargs='?', default=120)
parser.add_argument('-c1', dest='classI',   help='Discover classI lantibiotics default=True [True|False]',  nargs='?', default=True)
parser.add_argument('-c2', dest='classII',  help='Discover classII lantibiotics default=True [True|False]', nargs='?', default=True)
parser.add_argument('-c3', dest='classIII', help='Discover classIII Bacteriocins default=True [True|False]',nargs='?', default=True)
parser.add_argument('-color', dest='genecolor', help='Gene color name or hexcode, default=lightgrey',nargs='?', default='lightgrey')
args, unknown = parser.parse_known_args()
args.cpu = str(args.cpu)
args.domT = str(args.domT)

# get the root of the bagel package
programdir = os.path.dirname(os.path.realpath(__file__))
programdir = programdir.replace("/scripts", "")


min_intergenic_len=40
max_intergenic_len=250

sORF_min = 60 # in bases
sORF_max = 180 
sORF_overlap = 30 

SCRATCHDIR='/tmp'
UNIPROTDIR='/data/FACoPv2_database/databases'
DIAMOND_DB='/data/FACoPv2_database/databases/uniprot_sprot.dmnd'
DIAMOND='/data/software/diamond/diamond/diamond'

extensions = ['.fna', '.txt', '.fasta']
complement_dict = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}
regexCleanDNA = re.compile('[^a-zA-Z]')  # allow only alphabet in sequences, # seq = regexCleanDNA.sub('', fasta[key])
regexCleanFilename = re.compile('[^a-zA-Z_]')  # allow only alphabet in filenames

AOI_discovery_HMMs= ['AOI_discovery_PFAM.hmm','AOI_discovery_TIGR.hmm','LinL.hmm']
AOI_annotation_context_HMMs = ['AOI_Annotation_Context_PFAM.hmm', 'AOI_Annotation_Context_TIGR.hmm' ,'LinL.hmm']
AOI_annotation_core_HMMs = ['AOI_Annotation_Core_BAGEL4.hmm', 'AOI_Annotation_Core_PFAM.hmm', 'AOI_Annotation_Core_TIGR.hmm','bagel4_bacteriocins.hmm' ]


ClassColor = { 'I': '#27a32d', 'II': '#1e8a23','III': '#086e0c' }


context_db_dict = {
	'all_context_db.faa': 1E-60
}

# Add selected bacteriocin classes
bacteriocins_AOI_discovery = {}
if str(args.classI)=='True' or str(args.classII)=='True': 
	bacteriocins_AOI_discovery['bagel5_class12_bacteriocins_db.faa'] = 1E-05
	bacteriocins_AOI_discovery['bagel5_class12_putative_bacteriocins_db.faa'] = 1E-05
if str(args.classIII)=='True': bacteriocins_AOI_discovery['bagel5_class3_bacteriocins_db.faa'] = 1E-21


# Protein databases for AOI annotation
bacteriocins_AOI_annotation = {  
	'bagel5_class12_bacteriocins_db.faa':1E-05,
	'bagel5_class12_putative_bacteriocins_db.faa':1E-06,
	'bagel5_class3_bacteriocins_db.faa':1E-21,
	'bagel5_RRE_db.faa': 1E-05
}

blastp_headers = [
    'query_id', 'subject_id', 'identity_percent', 'alignment_length','mismatches',
	'gap_opens', 'q_start', 'q_end', 's_start', 's_end','e_value', 'bit_score'
]

domT_headers = [
'query_id','access','tlen','query_name','subject_id','qlen','Evalue','bit_score','bias',
'ht','of','cEvalue','iEvalue','iscore','ibias',
'hmm_from','hmm_to','q_start','q_end','env_from','env_to','acc','description'
]	

domT_blastP_common_headers = ['subject_id','bit_score','realpos','tool','db']
AOI_headers =['refIndices','start','end', 'size', 'subject_ids']
gff_header  = ["chrom","db", "type", "start", "end", "name", "strand", "score", "description"]

AOI_Rules_df = pd.read_table(programdir+'/tables/bagel5_AOI_Rules_table.txt', sep='\t', comment='#')

# Bagel5 Annotation Tables
BacAnn_df = pd.read_table(programdir+'/tables/bagel5_bacteriocins_annotation.table', sep='\t')
CorePeptidesTypes_df = pd.read_table(programdir+'/tables/bagel5_CorePeptidesTypes.table', sep='\t')
ContextProteinTypes_df = pd.read_table(programdir+'/tables/bagel5_ContextProteinTypes.table', sep='\t')



##################################################################################################
##                             Functions                                                        ##
##################################################################################################

def run_cmd(cmd):
	result = ''
	result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
	#print("\tcommand stdout:", result.stdout)
	

def write_log(S):
	# write to console and logfile
	print(S)
	f = open(args.outdir+'/'+args.sessionid+'/bagel5.log', "a")
	f.write(S + '\n')
	f.close()

def load_dict_from_file(file_path):
    result_dict = {}
    with open(file_path, 'r') as file:
        for line in file:
            key, value = line.strip().split('\t', 1)
            result_dict[key] = value
    return result_dict



def read_fasta(filename):
	results = {}
	lines = open(filename, "r").read().split('\n')
	key=''
	seq=''
	for line in lines:
		if re.search("^>", line):
			if key != '': results[key] = seq
			my_list = re.match("^>(.*)", line)
			key = my_list.group(1)
			seq=''
		else:
		   seq += line
	if key != '': results[key] = seq  # add the last record
	return results

def inverse_complement(DNA):
	# add an N for unkown chars
	complemented_sequence = ''.join(complement_dict.get(base, 'N') for base in reversed(DNA))
	return complemented_sequence

def translate(DNA):
	protein = ''
	for i in range(0, len(DNA) - 2, 3):
		try:
			codon = DNA[i:i+3]
			protein += codons[codon]
		except KeyError:
			protein += 'x'
	return protein


def translate_2_orfs(DNA):
    protein = ''
    stop_codon = True
    for i in range(0, len(DNA) - 2, 3):
        codon = DNA[i:i+3]
        if stop_codon and codon in ["ATG", "TTG", "GTG"]: stop_codon = False
        if codon in codons and not stop_codon:
            protein += codons[codon]
        else:
            protein += 'x'
            stop_codon = True
    return protein

def add_linebreaks(S, linelength):
    result = ''
    for i in range(0, len(S), linelength): result += S[i:i + linelength] + '\n>'
    return result


def translate_2_orfs_6_frames(DNA):
	# new version of hmmsearch does not allow >100k, her we split DNA into fragments
	fragmentSize = 300000  # length in bases
	faa=''
	for strand in DNA:
		for i in range(0, len(DNA[strand]), fragmentSize):
			Fragment = DNA[strand][i:i+fragmentSize]
			j = 0 if strand == '+' else 3
			for frame in range(1,4): # Frames from upper strand
				translation = translate_2_orfs(Fragment[frame:])
				faa += '>frame_'+str(j+frame)+'_'+str(i+frame)+'-'+str(i+frame+len(Fragment))+'\n'
				faa += translation + '\n'
	return faa


def ReadMultiFasta(filename):
	fasta = read_fasta(filename)
	fasta_extended = {}   # key is extended filename+entryname, DNA is cleaned
	for key in fasta: 
		newkey = key.split(' ')[0]
		fasta_extended[newkey] = regexCleanDNA.sub('', fasta[key]).upper()
	return fasta_extended	

def determine_AOI(df):
	overlap_realpos = []
	AOIs = {}
	count=0
	prev_realpos = 0
	# find overlapping hits within the AOI size
	for index, row in df.sort_values('realpos').iterrows(): # Iterate over the sorted DataFrame rows
		if index > 0:
			if (int(row['realpos']) - int(prev_realpos)) < int(args.AOIsize):
				overlap_realpos.append(index)
			else:
				if len(overlap_realpos)>0:
					count+=1
					AOIs[count] = overlap_realpos 
					overlap_realpos = []
				overlap_realpos.append(index)
				prev_realpos = row['realpos'] 
		else: 
			prev_realpos = row['realpos'] # realpos of the first row
			overlap_realpos.append(index)
	
	
	# get the AOI borders with AOI size of at least int(args.AOIsize)
	AOI_df=pd.DataFrame(columns=AOI_headers)
	extension = int(int(args.AOIsize) / 2)
	for AOI in AOIs.values():
		if len(AOI)>1:
			subset = df.iloc[AOI]
			start = subset['realpos'].min()-extension
			end   = subset['realpos'].max()+extension
		else:
			start = df.iloc[AOI[0]]['realpos']-extension
			end   = df.iloc[AOI[0]]['realpos']+extension
		if start<1: start=1 

		#merge the overlapping subject_id's to one string 
		subject_ids = ', '.join(map(str, df.iloc[AOI]['subject_id']))
		AOI_df = AOI_df.append({'refIndices': AOI, 'start': start, 'end': end, 'size':  end-start, 'subject_ids': subject_ids}, ignore_index=True)
	AOI_df['start'] = AOI_df['start'].astype(int)
	AOI_df['end']   = AOI_df['end'].astype(int)
	return(AOI_df)


def Filter_on_AOI_rules(row):
	valid = 0
	for rules_index, rules_row in AOI_Rules_df.iterrows():
		if rules_row['HMM1'] in row['subject_ids'] and rules_row['HMM2'] in row['subject_ids']:
			#print(f"{rules_row['RiPP_Class']} found on the basis of {row['subject_ids']}")
			valid=1
			break
	return valid

def Filter_on_BLAST_hits(row):
	valid = 0
	if row['valid'] == 1: valid = 1
	else:	
		BlastHits = blastp_df['subject_id'].values
		if any(str(hit) in row['subject_ids'] for hit in BlastHits): valid=1
	return valid
	
def get_AOI_sequence(key, DNA, AOI_df):
	fna_str=''
	fna = {}
	for index, row in AOI_df.iterrows():
		fna_key = key+'_'+row['ID']
		fna_seq = DNA[row['start']:row['end']]
		fna_str += '>'+fna_key+'\n'+fna_seq+'\n'
		fna[fna_key] = fna_seq
	return fna_str, fna

def set_id(row):
    row['ID'] = 'AOI_'+str(1+row.name)
    return row

def get_GFF_locus_tag(row):
	items = re.match(".*locus_tag=(.*?);.*", row['description'])
	return items.group(1)	

def read_GFF(filename):
	GFF = pd.read_csv(filename, header=None,  comment='#',sep='\t', names=gff_header)
	convert_dict = { 'start': int, 'end': int }
	GFF = GFF.astype(convert_dict)  # be sure that start and end are integers
	# GFF = GFF.loc[GFF['type'] == 'gene'] # use only type gene
	GFF = GFF[GFF['type'].str.contains('gene|sORF')]
	GFF['locus_tag'] = GFF.apply(get_GFF_locus_tag, axis=1)
	return GFF

def GenePrediction(basename):
	# Call Prodigal
	cmd = 'prodigal -p meta -f gff -i '+basename+'.fna | '
	cmd += "sed 's/ID=/locus_tag=AOI_/'  | sed 's/CDS\s/gene	/' | sed 's/\"/#/g' > "
	cmd += basename+'.gff'
	run_cmd(cmd)
	
	# input -> basename.gff and basename.fna 
	GFF=read_GFF(basename+'.gff')

	FNA = ReadMultiFasta(basename+'.fna')
	prevGene = pd.DataFrame()
	fnn = ''
	faa = ''
	intergenic = ''
	first = True
	for index, gene in GFF.sort_values(by=['chrom','start']).iterrows():
		seq = FNA[gene['chrom']][gene['start']-1:gene['end']]
		if gene['strand'] == '-': seq = inverse_complement(seq)
		prot = translate_2_orfs(seq)
		fnn += '>'+gene['locus_tag']+'\n'+seq+'\n'
		faa += '>'+gene['locus_tag']+'\n'+prot+'\n'
		if first: first = False
		else:
			if gene['chrom'] == prevGene['chrom']: 
				intergenicSeq = FNA[gene['chrom']][prevGene['end']:gene['start']]
				if len(intergenicSeq) > min_intergenic_len and len(intergenicSeq) < max_intergenic_len:
					if prevGene['strand'] == '-': intergenic += '>'+prevGene['locus_tag']+'\n'+inverse_complement(intergenicSeq)+'\n'
					if gene['strand']     == '+': intergenic += '>'+gene['locus_tag']+'\n'+intergenicSeq+'\n'
		prevGene=gene
	with open(basename+'.fnn', 'w') as file: file.write(fnn)
	with open(basename+'.faa', 'w') as file: file.write(faa)
	with open(basename+'.intergenic.fnn', 'w') as file: file.write(intergenic)


def Diamond_Annotation(basename):
	# call DIAMOND
	cmd =  DIAMOND+' blastp --unal 1 --threads '+args.cpu+' --tmpdir '+SCRATCHDIR +' --db '+DIAMOND_DB
	cmd += ' --evalue 0.01 --max-target-seqs 1 '
	cmd += '--query '+basename+'.all.faa --out '+basename+'.diamond.tab'
	run_cmd(cmd)
	cmd = 'python3 '+programdir+'/scripts/diamond_format_results.py -diamond '+basename+'.diamond.tab -db '+UNIPROTDIR+'/uniprot_sprot.description -out '+basename+'.FACoP.table'
	run_cmd(cmd)
	return pd.read_csv(basename+'.FACoP.table', sep='\t')

def predict_sORF(basename):
	path = os.path.dirname(basename)
	filename = os.path.basename(basename)
	cmd = 'python3 '+programdir+'/scripts/sORF_prediction.py -s '+path+' -basename '+filename
	#print(cmd)
	run_cmd(cmd)

def MergeProdigal_sORF(basename):
	sORF = read_fasta(basename+'.sORF.fnn')
	faa = ''
	for key in sORF: faa += '>'+key+'\n'+translate_2_orfs(sORF[key])+'\n'
	with open(basename+'.sORF.faa', 'w') as file: file.write(faa)
	cmd = 'cat '+basename+'.sORF.fnn '+basename+'.fnn >'+basename+'.all.fnn'
	run_cmd(cmd)
	cmd = 'cat '+basename+'.sORF.faa '+basename+'.faa >'+basename+'.all.faa'
	run_cmd(cmd)
	cmd = 'cat '+basename+'.sORF.gff '+basename+'.gff >'+basename+'.all.gff'
	run_cmd(cmd)
	return read_GFF(basename+'.all.gff')

def CorePeptidesHMM(basename):
	path=basename+'_HMM'
	if not os.path.exists(path): os.mkdir(path)
	domT_df=pd.DataFrame(columns=domT_headers)
	allORFs=read_fasta(basename+'.all.faa')
	for HMM in AOI_annotation_core_HMMs:
		domT_outputfile = path+'/'+HMM+'.domtbl'
		cmd = 'hmmsearch --noali --acc --cpu '+args.cpu
		cmd += ' --domtblout '+domT_outputfile
		cmd += ' --domT '+args.domT+' '+programdir+'/db_hmm/'+HMM+' '
		cmd += basename+'.all.faa >> '+ path+'/hmmsearch_CorePeptides.log'
		run_cmd(cmd)
		if os.path.exists(domT_outputfile) and os.path.getsize(domT_outputfile) > 0:
			tmp_df = pd.read_csv(domT_outputfile, sep='\s+', comment='#', header=None, names=domT_headers)
			tmp_df['db']=HMM
			domT_df = pd.concat([domT_df, tmp_df], axis=0, ignore_index=True)
	return domT_df		

def ContextHMM(basename):
	path=basename+'_HMM'
	if not os.path.exists(path): os.mkdir(path)
	domT_df=pd.DataFrame(columns=domT_headers)
	allORFs=read_fasta(basename+'.all.faa')
	for HMM in AOI_annotation_context_HMMs:
		domT_outputfile = path+'/'+HMM+'.domtbl'
		cmd = 'hmmsearch --noali --acc --cpu '+args.cpu
		cmd += ' --domtblout '+domT_outputfile
		cmd += ' --domT '+args.domT+' '+programdir+'/db_hmm/'+HMM+' '
		cmd += basename+'.all.faa >> '+ path+'/hmmsearch_Context.log'
		run_cmd(cmd)
		if os.path.exists(domT_outputfile) and os.path.getsize(domT_outputfile) > 0:
			tmp_df = pd.read_csv(domT_outputfile, sep='\s+', comment='#', header=None, names=domT_headers)
			tmp_df['db']=HMM
			domT_df = pd.concat([domT_df, tmp_df], axis=0, ignore_index=True)
	return domT_df		

def AOIblast(basename):
	blastp_df=pd.DataFrame(columns=blastp_headers)
	blastp_df['db']=''
	for db, evalue in bacteriocins_AOI_annotation.items():
		write_log('\t\t BlastP: '+db)
		blast_outputfile = basename+'.'+db+'.blastp'
		cmd  = 'blastp -outfmt 6 -max_target_seqs 1 -num_threads '+args.cpu
		cmd += ' -db '+programdir+'/db_proteins/'+db
		cmd += ' -query '+ basename+'.all.faa'
		cmd += ' -evalue '+str(evalue)
		cmd += ' -out '+ blast_outputfile
		run_cmd(cmd)
		if os.path.exists(blast_outputfile) and os.path.getsize(blast_outputfile) > 0:
			tmp_df = pd.read_csv(blast_outputfile, delimiter='\t', comment='#', header=None, names=blastp_headers)
			tmp_df['db']=db
			blastp_df = pd.concat([blastp_df, tmp_df], axis=0, ignore_index=True)
	return blastp_df

def Add_UniProt_features(basename):
	# the default database for blast_bacteriocins.py is bagel5_class12_bacteriocins_db.faa
	# this function generates basename.lanti.features eith alignment and modifications
	cmd = 'python3 '+programdir+'/scripts/blast_bacteriocins.py -query '+basename
	run_cmd(cmd)

def process_blastp(row, lenDNA):
	items=re.split("_|-",row['query_id'])                             # e.g., row['query_id']=frame_1_100001-200000  ==> frame, 1, 100001, 200000
	strand = '+' if items[1] in ['1', '2', '3'] else '-'              # Determine the reading frame
	if strand == '+': realpos = int(items[2])+3*int(row['q_start'])   # real DNA pos = header + 3*proteinPos
	else: realpos = lenDNA-int(items[2])-3*int(row['q_start'])	      # real DNA pos = length DNA - header - 3*proteinPos 
	return realpos

def HMMSEARCH_six_frames(key,outpath, lenDNA):
	domT_df=pd.DataFrame(columns=domT_headers)
	domT_df.drop(domT_df.index, inplace=True)  # remove old entries
	
	for HMM in AOI_discovery_HMMs:
		domT_outputfile = outpath+'/domtblout/'+key+'__'+HMM+'.domtbl'
		cmd = 'hmmsearch --noali --acc --cpu '+args.cpu
		cmd += ' --domtblout '+domT_outputfile
		cmd += ' --domT '+args.domT+' '+programdir+'/db_hmm/'+HMM+' '
		cmd += outpath+'/sixframes/'+key+'__sixframes.faa >> '+ outpath+'/domtblout/hmmsearch.log'
		run_cmd(cmd)
		if os.path.exists(domT_outputfile) and os.path.getsize(domT_outputfile) > 0:
			tmp_df = pd.read_csv(domT_outputfile, sep='\s+', comment='#', header=None, names=domT_headers)
			tmp_df['db']=HMM
			domT_df = pd.concat([domT_df, tmp_df], axis=0, ignore_index=True)
	if not domT_df.empty: 
		domT_df['realpos'] = domT_df.apply(process_blastp, args=(lenDNA,), axis=1)
		domT_df['tool']='hmm'
		domT_df.sort_values('realpos').to_csv(outpath+'/domtblout/'+key+'__domT.table', sep='\t', index=False)
	else: # make empty df to be able to merge domT
		domT_df = domT_df.reindex(columns = domT_blastP_common_headers)
	return domT_df

def BLAST_six_frames(key, outpath, lenDNA):
	blastp_df=pd.DataFrame(columns=blastp_headers)
	blastp_df.drop(blastp_df.index, inplace=True)  # remove old entries
	blastp_df['db']=''
	for db, evalue in bacteriocins_AOI_discovery.items():
		write_log('\t\t BlastP: '+db)
		blast_outputfile = outpath+'/blast/'+key+'__'+db+'.blastp'
		cmd  = 'blastp -outfmt 6 -max_target_seqs 1 -num_threads '+args.cpu
		cmd += ' -db '+programdir+'/db_proteins/'+db
		cmd += ' -query '+ outpath+'/sixframes/'+key+'__sixframes.faa'
		cmd += ' -evalue '+str(evalue)
		cmd += ' -out '+ blast_outputfile
		run_cmd(cmd)
		if os.path.exists(blast_outputfile) and os.path.getsize(blast_outputfile) > 0:
			tmp_df = pd.read_csv(blast_outputfile, delimiter='\t', comment='#', header=None, names=blastp_headers)
			tmp_df['db']=db
			blastp_df = pd.concat([blastp_df, tmp_df], axis=0, ignore_index=True)
	if not blastp_df.empty: 
		blastp_df['realpos'] = blastp_df.apply(process_blastp, args=(lenDNA,), axis=1)
		blastp_df['tool']='blast'
		blastp_df.sort_values('realpos').to_csv(outpath+'/blast/'+key+'__BLASTP.table', sep='\t', index=False)
	else: # make empty df to be able to merge domT
		blastp_df = blastp_df.reindex(columns = domT_blastP_common_headers)
	return blastp_df	

def remove_duplicates(cell_value): # Function to remove duplicates within a df cell
    values = cell_value.split(',')
    unique_values = list(set(values))
    return ','.join(unique_values)


def add_bacteriocin_annotation(row):
	BacAnn_row = BacAnn_df.loc[BacAnn_df['ID'] == row['CoreBlast']]
	if not BacAnn_row.empty:
		Name           = BacAnn_row['Name'].iloc[0]
		StructuralGene = BacAnn_row['StructuralGene'].iloc[0]
		Class          = BacAnn_row['Class'].iloc[0]
		SubClass       = BacAnn_row['SubClass'].iloc[0]
	else:
		Name           = ''
		StructuralGene = ''
		Class          = ''
		SubClass       = ''
	return pd.Series([Name, StructuralGene, Class, SubClass], index=['Name', 'StructuralGene', 'Class', 'SubClass'])

def contains_C_and_SerThr(row):
	if len(row['Peptide']) > 1: 
		if 'C' in row['Peptide'] and re.search(r'S|T', row['Peptide']):
			return True
	return False

def add_peptide_seq(row, FNA):
	peptide = ''
	try:
		DNA = FNA[row['chrom']]
		gene = DNA[row['start']-1:row['end']]
		if row['strand'] == '-': gene = inverse_complement(gene)
		if len(gene) < args.PepSize*3: peptide = translate(gene)
	except:
		peptide=''
	return peptide	

def row2gff(row):
	#gff_header  = ["chrom","db", "type", "start", "end", "name", "strand", "score", "description"]
	description =  'ID='+row['locus_tag'] 
	items = ['locus_tag', 'gene', 'type','product', 'Name', 'StructuralGene', 'Class', 'SubClass','Type','Peptide']
	for item in items: 
		if row[item] != '': description += ';' + item + '=' + str(row[item])
	return '\t'.join([row['chrom'],'bagel5',str(row['type']),str(row['start']),str(row['end']),'.',row['strand'],str(row['color']),description])


def ExportAsGFF(df, filename):
	df['GFF'] = df.apply(row2gff, axis=1)
	with open(filename, 'w') as f: f.write('\n'.join(df['GFF'].astype(str)))


def add_ContextProteinType(row):
	Type=row['Type']
	color=row['color']
	context_hmm_values = row['ContextHMM'].split(',')
	# Check if any of the values are present in the 'HMM' column of ContextProteinTypes_df
	filtered_df = ContextProteinTypes_df[ContextProteinTypes_df['HMM'].isin(context_hmm_values)]
	if not filtered_df.empty:
		desired_row = filtered_df.iloc[0]
		Type=desired_row['Type']
		color=desired_row['Color']
	return pd.Series([Type, color])

	

def add_CorePeptidesType(row):
	Type=row['Type']
	color=row['color']
	core_hmm_values = row['CoreHMM'].split(',')
	filtered_df = CorePeptidesTypes_df[CorePeptidesTypes_df['HMM'].isin(core_hmm_values)]
	if not filtered_df.empty:
		desired_row = filtered_df.iloc[0]
		Type=desired_row['Type']
		color=desired_row['Color']
	else:
		if row['Class'] == 'I': color = ClassColor['I']
		if row['Class'] == 'II': color = ClassColor['II']
		if row['Class'] == 'III': color = ClassColor['III']
		if row['Class'] != '': 	Type='Core-'+ row['Class']
	return pd.Series([Type, color])

def PEPMatch(basename):
	mismatch = 1
	kmer = 5
	directory = os.path.dirname(basename)
	filename = os.path.basename(basename)
	cmd = 'bash '+programdir+'/scripts/PEPMatch.sh '+programdir+'/db_proteins/bagel5_fragments.faa '+directory
	cmd += ' '+filename+' '+str(args.cpu)+' '+str(mismatch)+' '+str(kmer) 
	run_cmd(cmd)
	if os.path.exists(basename+'.pepmatch.tab'):
		df = pd.read_csv(basename+'.pepmatch.tab', delimiter='\t', comment='#', header=0, names=['fragment','sequence','locus_tag','mismatches','start','end'])
		df['locus_tag'] = df['locus_tag'].str.replace(r'\.1$', '', regex=True)  #  remove the unwanted tail  .1  to locus_tag which is added by PEPMatch
		return df
	else:
		return pd.DataFrame(columns=['fragment','sequence','locus_tag','mismatches','start','end'])

def add_PEPMatch(row, PEPMatch_df):
	if not PEPMatch_df.empty:
		hits = PEPMatch_df[PEPMatch_df['locus_tag'] == row['locus_tag']]
		if not hits.empty:
			return hits.iloc[0]['fragment']+'['+str(hits.iloc[0]['start'])+'-'+str(hits.iloc[0]['end'])+']'
		else:
			return ''
	else:
		return ''


##################################################################################################
##                             MAIN                                                             ##
##################################################################################################

codons = load_dict_from_file(programdir+'/tables/codons.txt')


# Get a list of files with the proper extensions
FullFilenames = []
for extension in extensions: FullFilenames.extend(glob.glob(f'{args.queryfolder}/*{extension}'))

Filenames = [os.path.basename(item) for item in FullFilenames] # Remove the path from each file in the list

if not os.path.exists(args.outdir): os.mkdir(args.outdir)
if not os.path.exists(args.outdir+'/'+args.sessionid): os.mkdir(args.outdir+'/'+args.sessionid)

# Read an empty html for Bootstrap content
with open(programdir+'/html/html_top.txt', 'r') as file:  html_top = file.read()
with open(programdir+'/html/html_bottom.txt', 'r') as file:  html_bottom = file.read()

TableOfContent_df = pd.DataFrame(columns=['filename','key','outpath','url','sessionid','basename','table','gff'])
# Screen all files and fasta entries
for filename in sorted(Filenames):
	write_log('Analyzing: '+filename)
	fasta = ReadMultiFasta(args.queryfolder+'/'+filename) 

	outpath = os.path.basename(filename)
	url = args.sessionid+'/'+os.path.splitext(outpath)[0]
	outpath = args.outdir+'/'+args.sessionid+'/'+os.path.splitext(outpath)[0]
	if not os.path.exists(outpath): os.mkdir(outpath)
	if not os.path.exists(outpath+'/domtblout'): os.mkdir(outpath+'/domtblout')
	if not os.path.exists(outpath+'/blast'):     os.mkdir(outpath+'/blast')
	if not os.path.exists(outpath+'/sixframes'): os.mkdir(outpath+'/sixframes')

	
	# screen all fasta entries for AOIs 
	for key in fasta: # key is extended filename+entryname, DNA and filename are cleaned
		if len(fasta[key]) < args.minAOIsize: continue  # Scaffold too small, move to the next iteration of the loop	
		DNA = {}
		DNA['+']=fasta[key]
		DNA['-']=inverse_complement(fasta[key])

		write_log('\t Translate to six frames: '+key +' length='+str(len(DNA['+'])))
		faa = translate_2_orfs_6_frames(DNA)
		with open(outpath+'/sixframes/'+key+'__sixframes.faa', 'w') as f: f.write(faa)		

		# HMMSEARCH six frames 
		write_log('\t Search for AOIs using hmm models in '+key)
		domT_df = HMMSEARCH_six_frames(key, outpath, len(DNA['+']))

		# BLAST six frames 
		write_log('\t Search for AOIs using Blastp bacteriocin proteins in '+key)
		blastp_df = BLAST_six_frames(key, outpath, len(DNA['+']))
			
		# combine HMMSEARCH and BLAST six frames
		domT_blastP_hits_df = pd.concat([domT_df[domT_blastP_common_headers], blastp_df[domT_blastP_common_headers]], ignore_index=True)
		
		# Process the identified AOIs
		if not domT_blastP_hits_df.empty:
			
			if not os.path.exists(outpath+'/'+key): os.mkdir(outpath+'/'+key)
			basename=outpath+'/'+key+'/'+key+'__AOI'
			write_log('\tProcessing AOIs of '+key)
			domT_blastP_hits_df.sort_values('realpos').to_csv(basename+'.domT_blastP_hits.table', sep='\t', index=False)

			write_log('\tDefine the AOI areas in '+key)
			AOI_df = determine_AOI(domT_blastP_hits_df)
			if AOI_df.empty:
				write_log('\tNo Blast or HMM hits when Annotating AOIs '+key)
				continue  # Move to the next iteration of the loop			
			
			write_log('\t\tApply AOI filtering rules')
			AOI_df['valid'] = AOI_df.apply(Filter_on_AOI_rules, axis=1)
			AOI_df['valid'] = AOI_df.apply(Filter_on_BLAST_hits, axis=1)
			AOI_df = AOI_df[AOI_df['valid'] == 1]
			AOI_df.reset_index(drop=True, inplace=True)
			AOI_df = AOI_df.apply(set_id, axis=1)
			if AOI_df.empty:
				write_log('\tNo hits after applying AOI rules '+key)
				continue  # Move to the next iteration of the loop			
			AOI_df.sort_values('start').to_csv(basename+'.table', sep='\t', index=False)
			AOI_df[['ID','start','end']].sort_values(by=['start']).to_csv(basename+'.positions.table', sep='\t', index=False)

			
			write_log('\t\tWrite all AOI dna sequences to '+basename+'.fna')
			AOI_FNA_str, AOI_FNA = get_AOI_sequence(key, fasta[key], AOI_df) # return AOI_DNA_seq as string and list
			with open(basename+'.fna', 'w') as file: file.write(AOI_FNA_str)
			
			write_log('\t\tGene Prediction using Prodigal')
			GenePrediction(basename)

			write_log('\t\tSmall ORF Prediction using BAGEL5 sORF prediction')
			predict_sORF(basename)

			write_log('\t\tMerge Prodigal and sORF predictions')
			GFF_df = MergeProdigal_sORF(basename)

			write_log('\t\tAnnotation of proteins: DIAMOND UniProt')
			Diamond_df = Diamond_Annotation(basename)
			
			PEPMatch_df = PEPMatch(basename)
	
			write_log('\t\tAnnotation of Core Peptides: hmmsearch bagel5_CorePeptidesHMM')
			CorePeptidesHMM_df = CorePeptidesHMM(basename)
			CorePeptidesHMM_df = CorePeptidesHMM_df.rename(columns={'query_id': 'locus_tag', 'query_name': 'CoreName', 'subject_id': 'CoreHMM'})
			
			
			write_log('\t\tAnnotation of Core Peptides: Blastp core peptides Experimentally described')
			CorePeptidesBlast_df = AOIblast(basename)
			CorePeptidesBlast_df = CorePeptidesBlast_df.rename(columns={'query_id': 'locus_tag', 'subject_id': 'CoreBlast'})
			
			write_log('\t\tAdd UniProt features for lantibiotics,')
			Add_UniProt_features(basename)
			
			
			write_log('\t\tAnnotation of Context Proteins: hmmsearch using bagel5_ContextHMM')
			ContextHMM_df = ContextHMM(basename)
			ContextHMM_df = ContextHMM_df.rename(columns={'query_id': 'locus_tag', 'query_name': 'ContextName', 'subject_id': 'ContextHMM'})

			write_log('\t\tMerge all Annotations of AOI proteins')
			Annotation_df = GFF_df[['locus_tag', 'chrom', 'type','start', 'end', 'strand']].copy()
			
			# Merge results
			Annotation_df = pd.merge(Annotation_df, Diamond_df[['locus_tag', 'product', 'gene']], on='locus_tag', how='outer')
			Annotation_df = pd.merge(Annotation_df, CorePeptidesHMM_df[['locus_tag', 'CoreName', 'CoreHMM']], on='locus_tag', how='outer')
			Annotation_df = pd.merge(Annotation_df, CorePeptidesBlast_df[['locus_tag', 'CoreBlast']], on='locus_tag', how='outer')
			Annotation_df = pd.merge(Annotation_df, ContextHMM_df[['locus_tag', 'ContextName', 'ContextHMM']], on='locus_tag', how='outer')
			Annotation_df.sort_values(by=['chrom','start']).to_csv(basename+'.Annotation.table', sep='\t', index=False)




			##  Clean the AOI Annotation table  ##
			# a) Merge multiple locus_tag hits
			non_agg_columns = ['chrom','type','start','end', 'strand','gene','product']  # Columns to take the first value
			agg_columns = ['CoreName','CoreHMM','ContextName','ContextHMM','CoreBlast']  # Columns to aggregate 
			agg_dict = {col: 'first' if col in non_agg_columns else ','.join for col in non_agg_columns + agg_columns}
			Annotation_df = Annotation_df.fillna("").astype(str).groupby('locus_tag').agg(agg_dict).reset_index()
			# b) Select only AOIs with hits on CoreHMM and/or coreBlast
			agg_columns = ['CoreHMM','CoreBlast']  # Columns to aggregate 
			agg_dict = {' '.join for col in agg_columns}
			df = Annotation_df.fillna("").astype(str).groupby('chrom').agg(agg_dict).reset_index()
			df.replace({' ': '', ',': ''}, regex=True, inplace=True)  # clean the strings
			df['combi'] = df.apply(lambda row: row['CoreHMM']+row['CoreBlast'], axis=1)
			df['len_combi'] = df['combi'].apply(lambda x: len(str(x)))
			# c) In Annotation Mode remove AOIs whithout hits with CoreHMM and/or CoreBlast.
			AOI_chrom_list = df.loc[df['len_combi'] > 3, 'chrom'].tolist() # get the list of AOIs with hits
			if args.mode == 'A': Annotation_df = Annotation_df[Annotation_df['chrom'].isin(AOI_chrom_list)]
			# d) remove replicates
			Annotation_df = Annotation_df.applymap(remove_duplicates)

			
			# e) exit if empty
			if Annotation_df.empty:
				write_log('\tNo core peptide hits with CoreHMM and/or CoreBlast '+key)
				continue  # Move to the next iteration of the loop			

			# Add bacteriocin annotation data from BacAnn_df
			Annotation_df[['Name','StructuralGene','Class','SubClass']] = Annotation_df.apply(add_bacteriocin_annotation, axis=1)
			Annotation_df['start'] = pd.to_numeric(Annotation_df['start'],errors='coerce').astype('Int64')
			Annotation_df['end']   = pd.to_numeric(Annotation_df['end'],  errors='coerce').astype('Int64')

			write_log('\t\tAdd peptide sequences')
			Annotation_df['Peptide']   = Annotation_df.apply(add_peptide_seq, args=(AOI_FNA,), axis=1)
			write_log('\t\tCheck for Cys and SerThr content of the peptide')
			Annotation_df['CysSerThr'] = Annotation_df.apply(contains_C_and_SerThr, axis=1)
			Annotation_df['PEPMatch']  = Annotation_df.apply(add_PEPMatch, args=(PEPMatch_df,), axis=1) 

			
			# Add Context Protein Type and color
			Annotation_df['color'] = args.genecolor
			Annotation_df['Type'] = ''
			Annotation_df[['Type','color']] = Annotation_df.apply(add_ContextProteinType, axis=1)
			Annotation_df[['Type','color']] = Annotation_df.apply(add_CorePeptidesType, axis=1)
			
			# Prepare export
			Annotation_df.sort_values(by=['chrom','start']).to_csv(basename+'.Annotation.Clean.table', sep='\t', index=False)
			
			# EXPORT FINAL result to session root
			Annotation_df.sort_values(by=['chrom','start']).to_csv(outpath+'/'+key+'.annotation.table', sep='\t', index=False)
			html_table = Annotation_df.sort_values(by=['chrom', 'start']).to_html(index=False, classes='table table-striped table-sm')
			with open(outpath+'/'+key+'.annotation.table.html', 'w') as f:	f.write(html_top+html_table+html_bottom)
	
			# Export GFF which is the source for web server visualization
			ExportAsGFF(Annotation_df, outpath+'/'+key+'.annotation.gff')
			
			# Creating a new row to the combined TOC
			TableOfContent_row = {
				'filename': filename,
				'key': key,
				'outpath': outpath,
				'url': url+'/'+key,
				'sessionid': args.sessionid,
				'basename': key+'__AOI',
				'table': key+'.annotation.table',
				'gff': key+'.annotation.gff'
			}

			# Appending the new row to the DataFrame
			TableOfContent_df = TableOfContent_df.append(TableOfContent_row, ignore_index=True)
			

TableOfContent_df.sort_values(by=['filename','key']).to_csv(args.outdir+'/'+args.sessionid+'/TableOfContent.txt', sep='\t', index=False)
print(args.outdir+'/'+args.sessionid+'/TableOfContent.txt')	
print(TableOfContent_df)	

with open(args.outdir+'/'+args.sessionid+'/sessionstop', "a") as f: f.write('session done|end|stop') ; # tell web server that run is done

